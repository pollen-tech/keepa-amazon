#!/usr/bin/env python3
"""
OPTIMIZED Daily Pipeline - Based on Expert Analysis
Cost: ~243 API calls instead of 24,289 (99% reduction!)
"""

import json
import os
import sys
import pandas as pd
import keepa
from datetime import datetime, timezone, date
from pathlib import Path
from google.cloud import bigquery
import requests
import warnings

warnings.simplefilter(action='ignore', category=FutureWarning)

# Add pipeline to path for imports
sys.path.append('pipeline')
from config.pipeline_config import (
    GCP_PROJECT_ID, GCP_DATASET_ID, GCP_TABLE_ID,
    KEEPA_API_KEY, TARGET_COLUMNS
)

# Domain mapping for Keepa API (corrected to use string codes)
DOMAIN_MAPPING = {
    "AmazonUS": ("US", "USD"),  # domain='US', currency=USD
    "AmazonGB": ("GB", "GBP"),  # domain='GB', currency=GBP
    "AmazonDE": ("DE", "EUR"),  # domain='DE', currency=EUR
    "AmazonJP": ("JP", "JPY"),  # domain='JP', currency=JPY
}

def get_fx_rates():
    """Get current FX rates to convert everything to USD"""
    try:
        # Using a free FX API (you could also use a paid service)
        response = requests.get("https://api.exchangerate-api.com/v4/latest/USD")
        rates = response.json()["rates"]
        
        # Convert to USD rates (inverted)
        fx_rates = {
            "USD": 1.0,
            "GBP": 1 / rates["GBP"],  # GBP to USD
            "EUR": 1 / rates["EUR"],  # EUR to USD  
            "JPY": 1 / rates["JPY"],  # JPY to USD
        }
        print(f"üìà FX Rates: GBP‚ÜíUSD={fx_rates['GBP']:.3f}, EUR‚ÜíUSD={fx_rates['EUR']:.3f}, JPY‚ÜíUSD={fx_rates['JPY']:.5f}")
        return fx_rates
    except Exception as e:
        print(f"‚ö†Ô∏è FX rate fetch failed: {e}. Using defaults.")
        return {"USD": 1.0, "GBP": 1.25, "EUR": 1.1, "JPY": 0.007}

def chunks(lst, n=100):
    """Split list into chunks of size n"""
    for i in range(0, len(lst), n):
        yield lst[i:i+n]

def rows_from_products(products, marketplace, category, fx_rate):
    """Extract rows from Keepa product data using optimized stats.current approach"""
    today = date.today()
    created_ts = datetime.now(timezone.utc)
    
    rows = []
    for p in products:
        try:
            # Use stats.current instead of parsing CSV time series!
            stats = p.get("stats", {})
            current = stats.get("current", [])
            
            # Extract current prices (in cents, convert to dollars)
            retail = current[3] / 100 if len(current) > 3 and current[3] > 0 else None
            discount = current[0] / 100 if len(current) > 0 and current[0] > 0 else None
            
            # Get rating from stats
            rating = stats.get("rating")
            if isinstance(rating, list) and rating:
                rating = rating[-1]  # Latest rating
            
            row = {
                "date": today,
                "retail_price": retail * fx_rate if retail else None,
                "discounted_price": discount * fx_rate if discount else None,
                "rating": rating,
                "asin": p["asin"],
                "marketplace": marketplace,
                "category": category,
                "created_at": created_ts,
                "ingestion_date": today,
            }
            rows.append(row)
            
        except Exception as e:
            print(f"  ‚ö†Ô∏è Error processing {p.get('asin', 'unknown')}: {e}")
            continue
            
    return rows

def fetch_optimized_prices(api, asin_data, fx_rates):
    """Fetch prices using optimized bulk API calls"""
    all_rows = []
    total_api_calls = 0
    total_asins_processed = 0
    
    print("üöÄ Starting optimized price fetch...")
    
    for domain_key, categories in asin_data.items():
        if domain_key not in DOMAIN_MAPPING:
            print(f"‚ö†Ô∏è Unknown domain: {domain_key}")
            continue
            
        domain_id, currency = DOMAIN_MAPPING[domain_key]
        fx_rate = fx_rates[currency]
        marketplace = domain_key.replace("Amazon", "")
        
        print(f"\nüåç Processing {marketplace} (domain='{domain_id}', FX={fx_rate:.3f}):")
        
        for category, asin_list in categories.items():
            if not asin_list:
                continue
                
            print(f"  üìÇ {category}: {len(asin_list)} ASINs")
            category_calls = 0
            category_rows = 0
            
            # Process in batches of 100 ASINs (expert recommendation)
            for batch in chunks(asin_list, 100):
                try:
                    print(f"    üîÑ Batch of {len(batch)} ASINs...")
                    
                    # OPTIMIZED API CALL - key changes:
                    # - history=False (no CSV time series)
                    # - stats=1 (include current prices)
                    # - rating=True (include rating data)
                    # - wait=True (respect rate limits)
                    products = api.query(
                        batch,
                        domain=domain_id,
                        history=False,  # üéØ KEY OPTIMIZATION: No historical data!
                        stats=1,        # üéØ Include current price stats
                        rating=True,    # Include rating data
                        wait=True       # Respect rate limits
                    )
                    
                    total_api_calls += 1
                    category_calls += 1
                    
                    # Extract data using optimized approach
                    batch_rows = rows_from_products(products, marketplace, category, fx_rate)
                    all_rows.extend(batch_rows)
                    category_rows += len(batch_rows)
                    total_asins_processed += len(batch)
                    
                    print(f"    ‚úÖ Got {len(batch_rows)} price records")
                    
                except Exception as e:
                    print(f"    ‚ùå Batch failed: {e}")
                    continue
            
            print(f"  üìä Category total: {category_calls} API calls, {category_rows} records")
    
    print(f"\nüìà OPTIMIZATION RESULTS:")
    print(f"   Total API calls: {total_api_calls} (vs 24,289 individual calls)")
    print(f"   Cost reduction: {(1 - total_api_calls/24289)*100:.1f}%")
    print(f"   ASINs processed: {total_asins_processed}")
    print(f"   Records collected: {len(all_rows)}")
    
    return all_rows

def upload_to_bigquery(client, rows):
    """Upload data to BigQuery"""
    if not rows:
        print("‚ö†Ô∏è No data to upload")
        return
        
    df = pd.DataFrame(rows)
    table_id = f"{GCP_PROJECT_ID}.{GCP_DATASET_ID}.{GCP_TABLE_ID}"
    
    print(f"üì§ Uploading {len(df)} records to BigQuery...")
    
    job_config = bigquery.LoadJobConfig(
        write_disposition="WRITE_APPEND",
        create_disposition="CREATE_NEVER",
    )
    
    try:
        job = client.load_table_from_dataframe(df, table_id, job_config=job_config)
        job.result()  # Wait for completion
        
        print(f"‚úÖ Successfully uploaded {len(df)} records!")
        
        # Get table stats
        table = client.get_table(table_id)
        print(f"   üìä Total table rows: {table.num_rows:,}")
        
    except Exception as e:
        print(f"‚ùå BigQuery upload failed: {e}")
        raise

def main():
    """Main optimized pipeline execution"""
    print("üöÄ OPTIMIZED Daily Amazon Keepa Price Pipeline")
    print("=" * 60)
    
    # Check environment
    if not KEEPA_API_KEY or KEEPA_API_KEY == "your-keepa-api-key":
        print("‚ùå Error: KEEPA_API_KEY not set properly in .env")
        return
    
    # Load ASIN data
    asin_path = Path('asins_fetch_via_scraping/asin_output_scraping/all_domains_top_asins.json')
    if not asin_path.exists():
        print(f"‚ùå ASIN file not found: {asin_path}")
        return
        
    with open(asin_path) as f:
        asin_data = json.load(f)
    
    # Get FX rates
    fx_rates = get_fx_rates()
    
    # Initialize Keepa API
    print(f"üîë Initializing Keepa API...")
    api = keepa.Keepa(KEEPA_API_KEY)
    
    # Fetch data using optimized approach
    rows = fetch_optimized_prices(api, asin_data, fx_rates)
    
    # Upload to BigQuery
    if rows:
        client = bigquery.Client(project=GCP_PROJECT_ID)
        upload_to_bigquery(client, rows)
    else:
        print("‚ùå No data collected")
        return
    
    print(f"\nüéâ OPTIMIZED PIPELINE COMPLETED!")
    print(f"üí∞ Estimated savings: ~99% cost reduction vs individual calls")
    print(f"‚ö° Ready for production deployment!")

if __name__ == "__main__":
    main() 